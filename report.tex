
% Default to the notebook output style
% Inherit from the specified cell style.
\documentclass[11pt]{article}
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{Udacity's Machine Learning Engineer Nanodegreee Capstone Project
Program \\ New York City Taxi Trip Duration}
	
	\author{Luciano Falqueto Santana}
   
  
     % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    \tableofcontents{}
    
\section{Introduction}

\hspace{0.5cm} 
The final goal for the Capstone Project is to solve one Kaggle Challenge, called \textbf{New York City Taxi Trip Duration}. The challenge proposal is to build a model capable of predicting the total duration of taxi trip in New York City using historic data provided in the Challenge page. The dataset includes pickup and drop-off time, pickup and drop-off coordinates, trip duration and number of passengers among other variables.

The dataset provided is already cleaned, allowing to direct focusing on the problem itself, the next step are analyze all the provided fields, make the feature engineering for the dataset, select the most relevant fields after pre-processing them, build the model and evaluate it.
   

\section{Problem Statement}\label{problem-statement}


\hspace{0.5cm} The competition dataset is based on the \href{https://cloud.google.com/bigquery/public-data/nyc-tlc-trips}{2016
NYC Yellow Cab trip record data} made available in Big Query on Google
Cloud Platform. The data was originally published by the
\href{http://www.nyc.gov/html/tlc/html/about/trip_record_data.shtml}{NYC
Taxi and Limousine Commission (TLC)}. The data was originally published by the
\href{http://www.nyc.gov/html/tlc/html/about/trip_record_data.shtml}{NYC
Taxi and Limousine Commission (TLC)}. \cite{challenge_site} The data was sampled and cleaned for the purposes of the challenge. 

Based the attributes of each trip present in the provided train dataset which also provide true values for the target variable \texttt{trip\_duration}, the goal is to predict the duration for each trip presetn and characterized in the test dataset.

\section{Dataset}

\hspace{0.5cm}  Two datasets are provided by the Challenge, the \textbf{Train} dataset with 1,458,644 trip records and the \textbf{Test} dataset with 625,134 trip records. Both datasets are in CSV format and were originally published by the NYC Taxi and Limousine Commission (TLC)\cite{challenge_site}.\\

\noindent The data fields, as seen in the challenge's website:

\begin{itemize}
\item \texttt{id} - a unique identifier for each trip
\item \texttt{vendor\_id} - a code indicating the provider associated with the trip record
\item \texttt{pickup\_datetime} - date and time when the meter was engaged
\item \texttt{dropoff\_datetime} - date and time when the meter was disengaged
\item \texttt{passenger\_count} - the number of passengers in the vehicle (driver entered value)
\item \texttt{pickup\_longitude} - the longitude where the meter was engaged
\item \texttt{pickup\_latitude} - the latitude where the meter was engaged
\item \texttt{dropoff\_longitude} - the longitude where the meter was disengaged
\item \texttt{dropoff\_latitude} - the latitude where the meter was disengaged
\item \texttt{store\_and\_fwd\_flag} - This flag indicates whether the trip record was held in vehicle memory before sending to the vendor because the vehicle did not have a connection to the server - Y=store and forward; N=not a store and forward trip
\item \texttt{{trip\_duration}} - duration of the trip in seconds
\end{itemize}

\noindent The Target Variable:\\

The challenge objective is to find values for trip\_duration for each trip in the \textbf{Test} dataset with a model trained using the information known from the \textbf{Train} dataset.

In more technical words as the origin data is already cleaned and prepared for processing, the first step to be done is the Feature Engineering, i.e. to analyze and prepare each field, select the most relevant ones. After this step, starts the data modeling phase and the model evaluation and fine tuning.
            
\section{Data Exploration and Feature engineering}

\hspace{0.5cm} For the data exploration we will be using \href{https://matplotlib.org/}{Matplotlib} and \href{https://seaborn.pydata.org/}{Seaborn} packets, besisdes the Pandas packet, which will be used throughout the whole project.

We will be exploring the data present in all the columns, understanding how the data is distributed and considering

The data provided was already processed and there are no null values in any of the columns as can be seen:

\begin{Verbatim}[commandchars=\\\{\}]
{\hspace{1.5cm}}
        vendor_id             0
        pickup_datetime       0
        dropoff_datetime      0
        passenger_count       0
        pickup_longitude      0
        pickup_latitude       0
        dropoff_longitude     0
        dropoff_latitude      0
        store_and_fwd_flag    0
        trip_duration         0
        dtype: int64
\end{Verbatim}

\subsection{Trip duration}

First we will describe the \texttt{trip\_duration} with simple statistical measurements like count, mean, standard deviation, quartlies and minimum and maximum values for the distribution:

\begin{Verbatim}[commandchars=\\\{\}]
{\hspace{1.5cm}} 
count   1458644.000000
mean        959.492273
std        5237.431724
min           1.000000
25%         397.000000
50%         662.000000
75%        1075.000000
max     3526282.000000
Name: trip_duration, dtype: float64
\end{Verbatim}

By the column description a few things come clear even though the data was pre-processed, no outliers were removed, that becomes quite clear by comparing the values of mean being much smaller than the std (standard deviation) and the maximum value for the distribution representing a trip almost 41 days long, which is not reasonable. A better way to confirm these assumptions is to plot the column values:

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_20_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
The graph confirm our assumption about the data, there are outliers in the distribution that must be removed before training the models, also as the distribution is skewed we will apply a log transform to the values in the column to achieve a better fit to the model.
\subsection{Geo Location Coordinates}

As we are speaking about coordinates, the best way to understanding the pickup and dropoff locations and how they are distributed in the dataset and check for outliers is a simple plot for each of the coordinates present in the data set:\texttt{pickup\_latitude}, \texttt{pickup\_longitude}, \texttt{dropoff\_latitude} and \texttt{dropoff\_longitude}:

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_25_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
The graph above show, as expected, most of the data is within the New York City bounding box, which is defined by the coordinates: (40.4774,-74.2589), ( 40.9176, -73.7004). However is also clear that there are a lot of outliers in the data, since there are latitude points ranging from around 32 to around 44 and longitude points from around -120 and around -60. Those values must be dropped before training the model

The lat-long coordinates can still be very useful, since some areas within the city have more traffic than others, impacting in the trip duration. To create zones inside the city, we will be using the lat-long for both the pickup place and dropoff place, but with the values rounded up to the third decimal place, with that we define areas in the city with the precision of approximately 111.32 m of radius \cite{lat_long_round_radius}
    
 \subsection{Distance and Average Speed}   
 
 Since the dataset provide a pickup point and a dropoff point, the obvious information that is possible to extract from that is the distance between the points, but since the distance is within a city and cities are formed by rectangular blocks, the distance between two points is better represented if we measure the \href{https://en.wikipedia.org/wiki/Taxicab_geometry}{taxicab metric or Manhattan distance}, which is, in a simple way, the sum of the distance in the latitude axis summed with the distance in the longitude axis.
 
 
 \begin{Verbatim}[commandchars=\\\{\}]
    {\color{incolor}In [{\color{incolor}66}]:} \PY{n}{NYC\PYZus{}LAT\PYZus{}KILOMETER\PYZus{}PER\PYZus{}DEGREE} \PY{o}{=} \PY{n}{geopy}\PY{o}{.}\PY{n}{distance}\PY{o}{.}\PY{n}{distance}\PY{p}{(}\PY{p}{(}\PY{l+m+mf}{40.7831}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mf}{73.9712}\PY{p}{)}\PY{p}{,}\PY{p}{(}\PY{l+m+mf}{41.7831}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mf}{73.9712}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{kilometers}
             \PY{n}{NYC\PYZus{}LON\PYZus{}KILOMETER\PYZus{}PER\PYZus{}DEGREE} \PY{o}{=} \PY{n}{geopy}\PY{o}{.}\PY{n}{distance}\PY{o}{.}\PY{n}{distance}\PY{p}{(}\PY{p}{(}\PY{l+m+mf}{40.7831}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mf}{73.9712}\PY{p}{)}\PY{p}{,}\PY{p}{(}\PY{l+m+mf}{40.7831}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mf}{72.9712}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{kilometers}
             
             \PY{n}{NYC\PYZus{}DEGREE\PYZus{}KM} \PY{o}{=} \PY{l+m+mf}{111.05938787411571}
             
             \PY{k}{def} \PY{n+nf}{calculate\PYZus{}city\PYZus{}block\PYZus{}distance}\PY{p}{(}\PY{n}{df\PYZus{}data}\PY{p}{)}\PY{p}{:}
                 \PY{n}{delta\PYZus{}lat} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{absolute}\PY{p}{(}\PY{n}{df\PYZus{}data}\PY{o}{.}\PY{n}{pickup\PYZus{}latitude} \PY{o}{\PYZhy{}} \PY{n}{df\PYZus{}data}\PY{o}{.}\PY{n}{dropoff\PYZus{}latitude}\PY{p}{)} \PY{o}{*} \PY{n}{NYC\PYZus{}LAT\PYZus{}KILOMETER\PYZus{}PER\PYZus{}DEGREE}    
                 \PY{n}{delta\PYZus{}lon} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{absolute}\PY{p}{(}\PY{n}{df\PYZus{}data}\PY{o}{.}\PY{n}{pickup\PYZus{}longitude} \PY{o}{\PYZhy{}} \PY{n}{df\PYZus{}data}\PY{o}{.}\PY{n}{dropoff\PYZus{}longitude}\PY{p}{)} \PY{o}{*} \PY{n}{NYC\PYZus{}LON\PYZus{}KILOMETER\PYZus{}PER\PYZus{}DEGREE}    
                 \PY{k}{return} \PY{n}{delta\PYZus{}lat} \PY{o}{+} \PY{n}{delta\PYZus{}lon}
             
             
             \PY{n}{df\PYZus{}train}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{distance}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{calculate\PYZus{}city\PYZus{}block\PYZus{}distance}\PY{p}{(}\PY{n}{df\PYZus{}train}\PY{p}{)}
             \PY{n}{df\PYZus{}train}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{avg\PYZus{}speed}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{df\PYZus{}train}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{distance}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{/}\PY{p}{(}\PY{n}{df\PYZus{}train}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{trip\PYZus{}duration}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{/}\PY{l+m+mi}{3600}\PY{p}{)}
    \end{Verbatim}
 
 After calculating the distance and by having the trip duration, we are able to calculate the average speed for each trip. After converting the trip duration to hours and dividing the distance by that value, we find the average speed for the trip.
 
 Since the two variables are extremely connected, we will shall analyze the graphs from both side by side.
 
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_25_0.png}
    \end{center}
    { \hspace*{\fill} \\}
 
 As can be seen, here we will also find outliers, from the distance side the graph show values from zero to around 1300km and, by the average speed side, show speeds as high as 12,000 km/h, a speed well above the Land speed record of 1,227.985 km/h \cite{{land_speed_record}
 
 The average speed cannot be used to create a feature, once it depends on the trip duration, which we want to calculate, thus it can only be used to filter values that will add spurious information to our model. That said the average speed will be dropped before training the model.

 \subsection{Picukp Datetime} 
 
 The date and time that the passenger was picked up is also a very important feature. 
 
 Regarding the hours, we can apply the same line of reasoning, as the hours of the day present a typical behavior regarding traffic, for example, all big cities have their "Rush Hour" during which you can find a lot of traffic. The graph bellow illustrate that well.
 
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_25_0.png}
    \end{center}
    { \hspace*{\fill} \\}
 
 Speaking about the days each day of the week presents a typical behaviour is of common sense, for example that in weekdays the traffic is heavier than in weekends, what affects the trip duration. In that same line, in holidays a lighter traffic is expected. Bellow are the graph for average trip duration each day of the week and a graph comparing the holidays to normal days.
 
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_25_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_25_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    
    
\subsection{Store and Forward} 

The store and forward is a Boolean that at first look may not give any information about the trip duration, but looking the graph bellow, it becomes clear there is a difference between the two. That difference may be explained by the technologies available to drivers like navigation apps, Internet or more modern cars.

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_25_0.png}
    \end{center}
    { \hspace*{\fill} \\}

\subsection{Vendor ID} 

A naive analysis could lead to a misconception regarding the Vendor ID as a not useful information, but given that technology has changed the private transportation market, a good routing or driver selection algorithm or payment method can make a difference in the trip_duration. In the whole dataset, there are only 2 different vendors and both present a very different behaviour in trip duration as can be seen in the graph bellow.

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_25_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
\subsection{Number of Passengers} 

In the same way as the previous analysis, it's worth to check the behavior of the number of passengers transported and if it might give more information about the trip duration.

As can be seen bellow, the graph of number of passengers shows a similar behavior for each number of passengers.

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_25_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
\section{Processing and filtering the Data}

\hspace{0.5cm} After knowing better our data, it's time to process it filtering the outliers and transforming the data to a more adequate form to train the models.

After creating some other features in the previous section,besides having the original set of columns vendor id, pickup datetime, dropoff datetime, passenger count, pickup longitude, pickup latitude, dropoff longitude, dropoff latitude, store and fwd flag and trip duration, we now some other features like distance, average speed, day of the week, hour of day and holiday.



The function also
\href{https://www.thoughtco.com/degree-of-latitude-and-longitude-distance-4070616}{coverts}
the respective latitude and longitude differences from degrees to
kilometers.

    \begin{Verbatim}[commandchars=\\\{\}]
{\hspace{1.5cm}} \PY{k}{def} \PY{n+nf}{L1\PYZus{}distance\PYZus{}2D}\PY{p}{(}\PY{n}{x1}\PY{p}{,} \PY{n}{y1}\PY{p}{,} \PY{n}{x2}\PY{p}{,} \PY{n}{y2}\PY{p}{)}\PY{p}{:}
            \PY{c+c1}{\PYZsh{} Latitude approximate conversion: degrees to kilometers}
            \PY{n}{x\PYZus{}conversion} \PY{o}{=} \PY{l+m+mi}{111}
            \PY{c+c1}{\PYZsh{} Longitude approximate conversion: degrees to kilometers}
            \PY{n}{y\PYZus{}conversion} \PY{o}{=}  \PY{l+m+mf}{111.321}
            \PY{c+c1}{\PYZsh{} Calculation of the Manhattan distance in kilometers}
            \PY{n}{dist} \PY{o}{=} \PY{n}{x\PYZus{}conversion}\PY{o}{*}\PY{n}{np}\PY{o}{.}\PY{n}{absolute}\PY{p}{(}\PY{n}{x2} \PY{o}{\PYZhy{}} \PY{n}{x1}\PY{p}{)} \PY{o}{+} \PY{n}{y\PYZus{}conversion}\PY{o}{*}\PY{n}{np}\PY{o}{.}\PY{n}{absolute}\PY{p}{(}\PY{n}{y2} \PY{o}{\PYZhy{}} \PY{n}{y1}\PY{p}{)}
            \PY{k}{return} \PY{n}{dist}
\end{Verbatim}

Then we calculate the Manhattan distance for each instance in
\texttt{train\_data} and store the information in a new variable called 
\texttt{travel\_distance\_km}. After that we will no longer need the coordinates
features.

    \begin{Verbatim}[commandchars=\\\{\}]
{\hspace{1.4cm}} \PY{n}{x1} \PY{o}{=} \PY{n}{train\PYZus{}data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{pickup\PYZus{}latitude}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
        \PY{n}{y1} \PY{o}{=} \PY{n}{train\PYZus{}data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{pickup\PYZus{}longitude}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
        \PY{n}{x2} \PY{o}{=} \PY{n}{train\PYZus{}data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{dropoff\PYZus{}latitude}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
        \PY{n}{y2} \PY{o}{=} \PY{n}{train\PYZus{}data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{dropoff\PYZus{}longitude}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
               
        \PY{n}{train\PYZus{}data} \PY{o}{=} \PY{n}{train\PYZus{}data}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{n}{columns}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{pickup\PYZus{}latitude}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{pickup\PYZus{}longitude}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
                                              \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{dropoff\PYZus{}latitude}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{dropoff\PYZus{}longitude}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
        
        \PY{n}{train\PYZus{}data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{travel\PYZus{}distance\PYZus{}km}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{L1\PYZus{}distance\PYZus{}2D}\PY{p}{(}\PY{n}{x1}\PY{p}{,} \PY{n}{y1}\PY{p}{,} \PY{n}{x2}\PY{p}{,} \PY{n}{y2}\PY{p}{)}
\end{Verbatim}

Naturally, we do the same for for \texttt{test\_data}.
    
    Once we have created the new feature \texttt{travel\_distance\_km}, we
can analyze its distribution, just like we did for
\texttt{trip\_duration}. Clearly the distribution is skewed, suggesting
we should apply a logarithmic transformation before training.

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_34_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
\subsection{Meaningless examples}\label{meaningless-examples}

\hspace{0.5cm} The figures above indicate we should be careful with outliers. From the
application of the \texttt{describe} method on \texttt{train\_data} we
can see that, strangely, at least one instance has the value zero for
\texttt{travel\_distance\_km}.

\begin{Verbatim}[commandchars=\\\{\}]
{\hspace{1.5cm}}        travel\_distance\_km
         count        1.458644e+06
         mean         5.102859e+00
         std          6.637813e+00
         min          0.000000e+00
         25\%          1.788463e+00
         50\%          3.043757e+00
         75\%          5.610539e+00
         max          1.435183e+03
\end{Verbatim}
            
    Taking a careful look on those cases, we note there are several rows
with \texttt{travel\_distance\_km} equals to zero.

    \begin{Verbatim}[commandchars=\\\{\}]
{\hspace{1.3cm}} \PY{n}{zero\PYZus{}travel\PYZus{}distance} \PY{o}{=} \PY{n}{train\PYZus{}data}\PY{o}{.}\PY{n}{loc}\PY{p}{[} \PY{n}{train\PYZus{}data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{travel\PYZus{}distance\PYZus{}km}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{==} \PY{l+m+mi}{0} \PY{p}{]}
        
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{zero\PYZus{}travel\PYZus{}distance}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
       
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
5897

    \end{Verbatim}
           
    Since those trips do not make sense we will throw them away. However, some data points in the train set are still not reliable, since the
magnitude of their values for the feature \texttt{trip\_duration} is of
order of ten thousands minutes.

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_42_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
             
    Although the taxi mean speed might hide important information about the
city traffic, it is certainly suspicious when its value is less then 0.1
km/h in travels up to 33 km.

    \begin{Verbatim}[commandchars=\\\{\}]
{\hspace{1.5cm}} \PY{n}{trip\PYZus{}duration\PYZus{}outliers} \PY{o}{=} \PY{n}{train\PYZus{}data}\PY{o}{.}\PY{n}{loc}\PY{p}{[} \PY{n}{train\PYZus{}data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{trip\PYZus{}duration}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{\PYZgt{}} \PY{l+m+mi}{10000}\PY{o}{*}\PY{l+m+mi}{60} \PY{p}{]}
         \PY{c+c1}{\PYZsh{} Mean speed (km/h)}
         \PY{l+m+mi}{3600}\PY{o}{*}\PY{n}{trip\PYZus{}duration\PYZus{}outliers}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{travel\PYZus{}distance\PYZus{}km}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{/} \PY{n}{trip\PYZus{}duration\PYZus{}outliers}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{trip\PYZus{}duration}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\hspace{1.5cm}} id
         id1864733    0.061220
         id0369307    0.017153
         id1325766    0.002934
         id0053347    0.033599
\end{Verbatim}
            
Because of that we remove from \texttt{train\_data} the lines shown in
\texttt{trip\_duration\_outliers}.

    The same analysis can be done looking for non-reliable data points from
the point of view of the travel distance, because the mean speed for those point 
does not make sense either.

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_49_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\hspace{1.5cm}} \PY{n}{travel\PYZus{}distance\PYZus{}outliers} \PY{o}{=} \PY{n}{train\PYZus{}data}\PY{o}{.}\PY{n}{loc}\PY{p}{[}  \PY{n}{train\PYZus{}data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{travel\PYZus{}distance\PYZus{}km}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{\PYZgt{}} \PY{l+m+mi}{300}\PY{p}{]}
         \PY{c+c1}{\PYZsh{} Mean speed (km/h)}
         \PY{l+m+mi}{3600}\PY{o}{*}\PY{n}{travel\PYZus{}distance\PYZus{}outliers}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{travel\PYZus{}distance\PYZus{}km}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{/} \PY{n}{travel\PYZus{}distance\PYZus{}outliers}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{trip\PYZus{}duration}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\hspace{1.5cm}} id
         id2306955     6211.686039
         id0978162     1851.813416
         id0116374     6710.694096
         id0982904     1235.223321
         id1146400    10567.491153
         id1001696     2421.114225
         id1510552     8456.072300
         id3626673     2589.658842
         id0838705     1168.140557
         id2644780     1065.878866
\end{Verbatim}
            
Removing from \texttt{train\_data} the lines shown in
\texttt{travel\_distance\_outliers} we have the new scatter plot.

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_56_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    There are more outliers with inconsistent mean speed values which can be
dropped from the training data.

    \begin{Verbatim}[commandchars=\\\{\}]
{\hspace{1.5cm}} \PY{n}{more\PYZus{}distance\PYZus{}outliers} \PY{o}{=} \PY{n}{train\PYZus{}data}\PY{o}{.}\PY{n}{loc}\PY{p}{[}  \PY{n}{train\PYZus{}data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{travel\PYZus{}distance\PYZus{}km}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{\PYZgt{}} \PY{l+m+mi}{150}\PY{p}{]}
         \PY{c+c1}{\PYZsh{} Mean speed (km/h)}
         \PY{n}{mean\PYZus{}speed\PYZus{}more} \PY{o}{=} \PY{l+m+mi}{3600}\PY{o}{*}\PY{n}{more\PYZus{}distance\PYZus{}outliers}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{travel\PYZus{}distance\PYZus{}km}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{/} \PY{n}{more\PYZus{}distance\PYZus{}outliers}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{trip\PYZus{}duration}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
         \PY{n}{mean\PYZus{}speed\PYZus{}more}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\hspace{1.5cm}} id
         id1092161      97.220917
         id1311087      43.787064
         id2778014    1351.784060
         id2066082     109.625331
         id0401529    1261.852560
         id0687776     116.109127
         id1216866     108.340772
         id3795134      46.182152
\end{Verbatim}

After throwing them away we can another look at the remaining points.

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_61_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsection{Rush hour and days in New York
City}\label{rush-hour-and-days-in-new-york-city}

\hspace{0.5cm} Naturally, rush hour periods and weekends could change taxi trip
duration. Since \texttt{pickup\_datetime} feature is a \texttt{datetime}
object, it is not difficult to label each instance with the
\href{https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.dt.weekday_name.html}{week
day} the trip was taken.

    \begin{Verbatim}[commandchars=\\\{\}]
{\hspace{1.6cm}} \PY{n}{train\PYZus{}data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{week\PYZus{}day}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{train\PYZus{}data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{pickup\PYZus{}datetime}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{dt}\PY{o}{.}\PY{n}{weekday\PYZus{}name}
         \PY{n}{test\PYZus{}data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{week\PYZus{}day}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{test\PYZus{}data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{pickup\PYZus{}datetime}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{dt}\PY{o}{.}\PY{n}{weekday\PYZus{}name}
\end{Verbatim}

    In many cities around the world there are reasonable well defined
periods of time called rush hours, on which one should not (if possible)
drive into the city. Usually rush hours take place take early in the
morning and in the end of afternoon.

In New York City, however, there are
\href{https://www.nytimes.com/1987/09/09/nyregion/new-york-rush-hours-grow-earlier-and-later.html}{longer
periods of rush hours}. Some people even claim that
\href{https://www.quora.com/What-times-are-considered-rush-hour-in-New-York-City}{all
times} are rush hours. Naturally, another important feature would be the
name of the street where the taxi trip begins. Rush hours periods near
\href{https://adventure.howstuffworks.com/new-york-city-guide1.htm}{airports},
for example, could be longer. Such deeper analysis could be performed
from latitude and longitude coordinates or even with the help of
\href{https://www.kaggle.com/maheshdadhich/strength-of-visualization-python-visuals-tutorial/data}{additional
datasets}, but it will not be carried here.

In order to eventually define a rush hour period and to analyze the
behavior of the data for different days of the week, we seek to
calculate the average trip duration per hour. First, we extract the hour
in which each trip started from the \texttt{datetime} objects, and
create a new variable called \texttt{pickup\_hour}.

    \begin{Verbatim}[commandchars=\\\{\}]
{\hspace{1.6cm}} \PY{n}{train\PYZus{}data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{pickup\PYZus{}hour}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{train\PYZus{}data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{pickup\PYZus{}datetime}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{dt}\PY{o}{.}\PY{n}{hour}
         \PY{n}{test\PYZus{}data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{pickup\PYZus{}hour}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{test\PYZus{}data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{pickup\PYZus{}datetime}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{dt}\PY{o}{.}\PY{n}{hour}
\end{Verbatim}

           
    Using pandas \texttt{groupby}
\href{https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html}{method}
we calculate the average values of \texttt{trip\_duration} for each
value of \texttt{pickup\_hour} and organize them considering each day of
the week. The results are then organized in a nice visualization plot using the \texttt{plotly}
\href{https://plot.ly/}{library}.
 
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{days.png}
    \end{center}
    { \hspace*{\fill} \\}

    
    The usual times attributed as rush hours are not clearly depicted in the
visualization plot. Actually, one could define rush hour as the period
between 8 am and 3 pm, on which there is a tendency to the taxi trip get
longer, on average, regardless the day of the week. However, that would
be quite an arbitrary choice, since it is based just on a trend. Another
one could come up with a different definition.

Although some exciting behaviors could be inferred from the figure, such
the ones in the early hours of Fridays and Saturdays, at the dawn of
Saturdays and Sundays, the every day peak around 10 pm, we choose to
keep all the days and hour variables and do not define any additional
label such as rush hour. It appears there is an important correlation
between all this features and we hope the chosen regression algorithms
are able to grab it.

    Since our analysis judges that most of the relevant information provided
by the feature \texttt{pickup\_datetime} can be compressed in
\texttt{week\_day} and \texttt{pickup\_hour}, we just drop
\texttt{pickup\_datetime}.

    \begin{Verbatim}[commandchars=\\\{\}]
{\hspace{1.6cm}} \PY{n}{train\PYZus{}data} \PY{o}{=} \PY{n}{train\PYZus{}data}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{n}{columns}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{pickup\PYZus{}datetime}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
         \PY{n}{train\PYZus{}data}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{p}{)}
\end{Verbatim}

            
    \subsubsection{\texorpdfstring{Provider associated with the trip record:
\texttt{vendor\_id}}{Provider associated with the trip record: vendor\_id}}\label{provider-associated-with-the-trip-record-vendor_id}

\hspace{0.5cm} A naive thinking could conclude that the taxi company choice does not
matter for taxi trip duration, since we are counting just the interval
between pickup and drop off. Some years ago this could have been true,
but nowadays (data is from 2016) there are many technological resources
such as \href{https://www.google.com/maps}{Google Maps} and
\href{https://www.waze.com/}{Waze} that a provider may or may not take
advantage. Additionally, providers which accept payment directly on an
smartphone app, similar to \href{https://www.uber.com}{Uber}, do not
loose time after the car reached its final destination.

A similar inspection to one made for \texttt{week\_day} can be carried
for the feature \texttt{vendor\_id}, which stores an id of the provider
associated with the trip record.

      \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{vendor.png}
    \end{center}
    { \hspace*{\fill} \\}


    
    On average it is clear that taxi trips on cars provided by the company
identified with number 2 take a little longer, which suggests
\texttt{vendor\_id} feature can play a relevant role in the prediction.
Then this feature will be kept.

\subsubsection{\texorpdfstring{Trips recorded in vehicle memory before
forward:
\texttt{store\_and\_fwd\_flag}}{Trips recording in vehicle memory before forward: store\_and\_fwd\_flag}}\label{trips-recording-in-vehicle-memory-before-forward-store_and_fwd_flag}

\hspace{0.5cm} The feature \texttt{store\_and\_fwd\_flag} is a flag indicating whether
the trip record was held in vehicle memory before sending to the vendor
because the vehicle did not have a connection to the server: \texttt{Y}
means store and forward while \texttt{N}not a store and forward trip.

Similarly to the discussion of the previous section, this feature could
indicate whether or not a car has technological resources available. A
resembling examination can be done for \texttt{store\_and\_fwd\_flag}.

      \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{stored.png}
    \end{center}
    { \hspace*{\fill} \\}

    
    Most of the trips, on average, are faster for those cars which do not
store information, perhaps indicating they have more technological
gadgets available, like wireless connection to the vendor. This suggests
\texttt{store\_and\_fwd\_flag} may be related with \texttt{vendor\_id}
and may play a relevant role in the prediction. Then this feature will
be kept.

\subsubsection{\texorpdfstring{The number of passengers in the vehicle:
\texttt{passenger\_count}}{The number of passengers in the vehicle: passenger\_count}}\label{the-number-of-passengers-in-the-vehicle-passenger_count}

\hspace{0.5cm} Now we perform a similar investigation for the feature which counts the
number of passengers the taxi cab was carrying.

     \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{passengers.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    
    
    The figure above shows a visual indication that there is indeed a
correlation between trip duration and number of passengers. Though some
fluctuations, the curves display reasonable behaviors, such as the one
for single passengers. It is fair to think that a single passenger is
faster to get in and get off a car.

    \subsection{One-hot encoding scheme for categorical
variables}\label{one-hot-encoding-scheme-for-categorical-variables}

    The one-hot encoding scheme should be applied on categorical variables.
It is important to note even \texttt{passenger\_count} should be one-hot
encoded, because if its values are treated as numbers, the algorithm
could consider during training non-integer values for the number of
passengers. 

    \begin{Verbatim}[commandchars=\\\{\}]
{\hspace{1.5cm}} \PY{n}{train\PYZus{}data} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{get\PYZus{}dummies}\PY{p}{(}\PY{n}{train\PYZus{}data}\PY{p}{,} \PY{n}{columns}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{vendor\PYZus{}id}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{passenger\PYZus{}count}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
                                             \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{store\PYZus{}and\PYZus{}fwd\PYZus{}flag}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{week\PYZus{}day}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{pickup\PYZus{}hour}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{train\PYZus{}data}\PY{o}{.}\PY{n}{columns}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Index(['trip\_duration', 'travel\_distance\_km', 'vendor\_id\_1', 'vendor\_id\_2',
       'passenger\_count\_0', 'passenger\_count\_1', 'passenger\_count\_2',
       'passenger\_count\_3', 'passenger\_count\_4', 'passenger\_count\_5',
       'passenger\_count\_6', 'store\_and\_fwd\_flag\_N', 'store\_and\_fwd\_flag\_Y',
       'week\_day\_Friday', 'week\_day\_Monday', 'week\_day\_Saturday',
       'week\_day\_Sunday', 'week\_day\_Thursday', 'week\_day\_Tuesday',
       'week\_day\_Wednesday', 'pickup\_hour\_0', 'pickup\_hour\_1', 'pickup\_hour\_2',
       'pickup\_hour\_3', 'pickup\_hour\_4', 'pickup\_hour\_5', 'pickup\_hour\_6',
       'pickup\_hour\_7', 'pickup\_hour\_8', 'pickup\_hour\_9', 'pickup\_hour\_10',
       'pickup\_hour\_11', 'pickup\_hour\_12', 'pickup\_hour\_13', 'pickup\_hour\_14',
       'pickup\_hour\_15', 'pickup\_hour\_16', 'pickup\_hour\_17', 'pickup\_hour\_18',
       'pickup\_hour\_19', 'pickup\_hour\_20', 'pickup\_hour\_21', 'pickup\_hour\_22',
       'pickup\_hour\_23'],
      dtype='object')

    \end{Verbatim}

         
    Naturally the same is done for \texttt{test\_data}.
            
    \subsection{Logarithmic
transformation}\label{logarithmic-transformation}

Accordingly to the data exploration part, before training we apply a
logarithmic transformation in the numerical variables.

    \begin{Verbatim}[commandchars=\\\{\}]
{\hspace{1.6cm}} \PY{n}{train\PYZus{}data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{trip\PYZus{}duration}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{n}{train\PYZus{}data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{trip\PYZus{}duration}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{)}
         \PY{n}{train\PYZus{}data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{travel\PYZus{}distance\PYZus{}km}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{n}{train\PYZus{}data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{travel\PYZus{}distance\PYZus{}km}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{)}
         
         \PY{n}{test\PYZus{}data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{travel\PYZus{}distance\PYZus{}km}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{n}{test\PYZus{}data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{travel\PYZus{}distance\PYZus{}km}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{)}
\end{Verbatim}

To avoid mistakes, we then rename the columns pointing explicitly they are been measured on
a logarithmic scale.

  \subsection{\texorpdfstring{Creating \texttt{X\_train},
\texttt{y\_train}}{Creating X\_train, y\_train and predicting on test\_data}}\label{creating-x_train-y_train-and-predicting-on-test_data}

\hspace{0.5cm} Before training the algorithms, we create the \texttt{X\_train} and
\texttt{y\_train} objects, while the later is defined to be the training
target variable.

    \begin{Verbatim}[commandchars=\\\{\}]
{\hspace{1.6cm}} \PY{n}{y\PYZus{}train} \PY{o}{=} \PY{n}{train\PYZus{}data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{log\PYZus{}trip\PYZus{}duration}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
         \PY{n}{X\PYZus{}train} \PY{o}{=} \PY{n}{train\PYZus{}data}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{n}{columns}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{log\PYZus{}trip\PYZus{}duration}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}

\section{Training: estimating Kaggle score using \texttt{train\_test\_split} from
\texttt{sklearn.model\_selection}}

\hspace{0.5cm} The competition does not provide direct access to the test targets, then
we are not able to calculate here the
\href{https://www.kaggle.com/c/nyc-taxi-trip-duration\#evaluation}{root
mean square logarithmic error} (RMSLE) \(\epsilon\), which is given by

\[ \epsilon = \sqrt{ \frac{1}{n} \sum_{i=1}^{n} ( \log(p_i + 1) - \log(a_i + 1)  )^{2}    }  \;,  \]
where \(n\) is the total number of observations in the test set, \(p_i\)
is the \(i\)-th prediction of trip duration, and \(a_i\) is the \(i\)-th
actual trip duration.

The RMSLE is similar to square root of the
\href{https://en.wikipedia.org/wiki/Mean_squared_error}{mean square
error}, which is the simplest evaluation metric and it has a
straightforward interpretation; it estimates the variance between the
predicted instances and correct ones. The lower this variance, the
better the prediction. Mean square logarithmic error is basically the
same measure but in a logarithmic scale. It is interesting if one does
not want to penalize too much huge differences between predictions and
actual values.

Our score is reveled only when we make a submission to Kaggle.

Naturally, it is not practical to make cross validation submitting files
in the competition's web site. Using \texttt{train\_test\_split} from
\texttt{sklearn.model\_selection} we can estimate the final Kaggle score
by selecting a fictitious test target from the training data.

    \begin{Verbatim}[commandchars=\\\{\}]
{\hspace{1.5cm}} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k}{import} \PY{n}{train\PYZus{}test\PYZus{}split}
         
         \PY{n}{XX\PYZus{}train}\PY{p}{,} \PY{n}{XX\PYZus{}test}\PY{p}{,} \PY{n}{yy\PYZus{}train}\PY{p}{,} \PY{n}{yy\PYZus{}test} \PY{o}{=} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,}
                                                                 \PY{n}{y\PYZus{}train}\PY{p}{,}
                                                                 \PY{n}{test\PYZus{}size} \PY{o}{=} \PY{l+m+mf}{0.3}\PY{p}{,}
                                                                 \PY{n}{random\PYZus{}state} \PY{o}{=} \PY{l+m+mi}{0}\PY{p}{)}
\end{Verbatim}


    Next we define a score function to perform cross validation.

    \begin{Verbatim}[commandchars=\\\{\}]
{\hspace{1.5cm}} \PY{k}{def} \PY{n+nf}{score\PYZus{}kaggle\PYZus{}log}\PY{p}{(}\PY{n}{y\PYZus{}true}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{)}\PY{p}{:}
             \PY{n}{y\PYZus{}pred\PYZus{}t} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{exp}\PY{p}{(}\PY{n}{y\PYZus{}pred}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{l+m+mi}{1}
             \PY{n}{y\PYZus{}true\PYZus{}t} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{exp}\PY{p}{(}\PY{n}{y\PYZus{}true}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{l+m+mi}{1}
             \PY{n}{e\PYZus{}i} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{square}\PY{p}{(} \PY{n}{np}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{n}{y\PYZus{}pred\PYZus{}t} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{n}{np}\PY{o}{.}\PY{n}{log}\PY{p}{(} \PY{n}{y\PYZus{}true\PYZus{}t} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{)} \PY{p}{)}
             \PY{n}{score} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(} \PY{p}{(}\PY{l+m+mi}{1}\PY{o}{/}\PY{n+nb}{len}\PY{p}{(}\PY{n}{y\PYZus{}true\PYZus{}t}\PY{p}{)}\PY{p}{)} \PY{o}{*} \PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{e\PYZus{}i}\PY{p}{)} \PY{p}{)}
             \PY{k}{return} \PY{n}{score}
\end{Verbatim}


    The \texttt{score\_kaggle\_log} function just applies the formula for
the
\href{https://www.kaggle.com/c/nyc-taxi-trip-duration\#evaluation}{root
mean square logarithmic error} (RMSLE), which is the evaluation metrics
considered in the competition. Note that, since we had applied a
logarithmic transformation in the numerical variables, we have put the
inverse log-transformation inside the score function.

\subsection{Linear Regression}\label{linear-regression}

\hspace{0.5cm} It is convenient to begin training models as simple as possible. Usually
this approach is helpful to give insights about which more complicated
path is worth to follow. Here we choose to start with the usual
multivariate
\href{https://en.wikipedia.org/wiki/Linear_regression}{linear
regression} with its standard
\href{http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html}{parameters}.

    \begin{Verbatim}[commandchars=\\\{\}]
{\hspace{1.5cm}} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{linear\PYZus{}model} \PY{k}{import} \PY{n}{LinearRegression}
         \PY{n}{model} \PY{o}{=} \PY{n}{LinearRegression}\PY{p}{(}\PY{p}{)}
         \PY{n}{clf} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{XX\PYZus{}train}\PY{p}{,} \PY{n}{yy\PYZus{}train}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Estimating RMSLE on the test set with LinearRegression}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{score\PYZus{}kaggle\PYZus{}log}\PY{p}{(} \PY{n}{yy\PYZus{}test}\PY{p}{,} \PY{n}{clf}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{XX\PYZus{}test}\PY{p}{)} \PY{p}{)} \PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Estimating RMSLE on the test set with LinearRegression
0.49269888205163326

    \end{Verbatim}

    If we take a look also in the RMSLE on the training set, there is no
sign of overfitting, suggesting that
\href{https://en.wikipedia.org/wiki/Regularization_(mathematics)}{regularization}
methods will not considerably improve the results.

    \begin{Verbatim}[commandchars=\\\{\}]
{\hspace{1.5cm}} \PY{n+nb}{print}\PY{p}{(}\PY{n}{score\PYZus{}kaggle\PYZus{}log}\PY{p}{(} \PY{n}{yy\PYZus{}train}\PY{p}{,} \PY{n}{clf}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{XX\PYZus{}train}\PY{p}{)} \PY{p}{)} \PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
0.4940908409128837

    \end{Verbatim}

    \subsection{\texorpdfstring{Linear regression with regularization:
\texttt{Lasso}}{Linear regression with regularization: Lasso}}\label{linear-regression-with-regularization-lasso}

\hspace{0.5cm} However, since some
\href{http://scikit-learn.org/stable/modules/linear_model.html}{generalized
linear models} are straightforward, we will take a quick look at some
regularized models. We begin by a multivariate linear model with
\href{https://dataorigami.net/blogs/napkin-folding/79033923-least-squares-regression-with-l1-penalty}{L1
penalty}, which is a penalty equals to the absolute value of the
magnitude of coefficients. In
\href{http://scikit-learn.org/stable/index.html}{scikit-learn} this
modified
\href{http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html}{model}
is imported as \texttt{Lasso}.

    \begin{Verbatim}[commandchars=\\\{\}]
{\hspace{1.5cm}} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{linear\PYZus{}model} \PY{k}{import} \PY{n}{Lasso}
         \PY{n}{model} \PY{o}{=} \PY{n}{Lasso}\PY{p}{(}\PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
         \PY{n}{clf} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{XX\PYZus{}train}\PY{p}{,} \PY{n}{yy\PYZus{}train}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Estimating RMSLE on the test set with Lasso}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{score\PYZus{}kaggle\PYZus{}log}\PY{p}{(} \PY{n}{yy\PYZus{}test}\PY{p}{,} \PY{n}{clf}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{XX\PYZus{}test}\PY{p}{)} \PY{p}{)} \PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Estimating RMSLE on the test set with Lasso
0.7835251024842801

    \end{Verbatim}

    Since L1 regularization limits the size of the coefficients, some of
them can become zero, therefore being eliminated. That is why this type
of model usually works well on sparse data, i.e. data with few relevant
coefficients. That is not the case here. Definitely, \texttt{Lasso} is
eliminating relevant coefficients during the training procedure, leading
to a poor performance.
            
    \subsection{\texorpdfstring{Linear regression with regularization:
\texttt{Ridge}}{Linear regression with regularization: Ridge}}\label{linear-regression-with-regularization-ridge}

\hspace{0.5cm} Now we considerer
\href{https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c}{L2
regularization}, which is a penalty equals to the square of the
magnitude of coefficients. As the square of a small number is a smaller
number, we do not expect to have as much null coefficients as we had
obtained with L1 regression. In
\href{http://scikit-learn.org/stable/index.html}{scikit-learn} a
\href{http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html}{linear
model with L2 regularization} is imported as \texttt{Ridge}.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}57}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{linear\PYZus{}model} \PY{k}{import} \PY{n}{Ridge}
         \PY{n}{model} \PY{o}{=} \PY{n}{Ridge}\PY{p}{(}\PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
         \PY{n}{clf} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{XX\PYZus{}train}\PY{p}{,} \PY{n}{yy\PYZus{}train}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Estimating RMSLE on the test set with Ridge}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{score\PYZus{}kaggle\PYZus{}log}\PY{p}{(} \PY{n}{yy\PYZus{}test}\PY{p}{,} \PY{n}{clf}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{XX\PYZus{}test}\PY{p}{)} \PY{p}{)} \PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Estimating RMSLE on the test set with Ridge
0.4926978071042146

    \end{Verbatim}
    
With the method \texttt{.coef\_} we can compare Ridge and Lasso coefficients. Regularization L1 leads to zero coefficients for all features, while L2 regularization is able to construct a model with more complexity.

   
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_120_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Between \texttt{Lasso} and \texttt{Ridge}, it is clean we should go on
with the later.

    \subsection{\texorpdfstring{Linear regression with regularization:
\texttt{ElasticNet}}{Linear regression with regularization: ElasticNet}}\label{linear-regression-with-regularization-elasticnet}

\hspace{0.5cm} If we had to chose one kind of regularization, there is no doubt about
what penalty should be chosen. Just for pedagogical purposes, we also
present the
\href{http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html}{method}
\texttt{ElasticNet} which combines L1 and L2 regularizations. An
interesting parameter is \texttt{l1\_ratio} which is a mixing parameter;
for \texttt{l1\_ratio\ =\ 0} the penalty is an L2 penalty. For
\texttt{l1\_ratio\ =\ 1} it is an L1 penalty and for
\texttt{0\ \textless{}\ l1\_ratio\ \textless{}\ 1}, the penalty is a
combination of L1 and L2.

Interestingly, a tiny contribution of an L1 term is enough to a poor
generalization.

    \begin{Verbatim}[commandchars=\\\{\}]
{\hspace{1.6cm}} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{linear\PYZus{}model} \PY{k}{import} \PY{n}{ElasticNet}
         \PY{n}{model} \PY{o}{=} \PY{n}{ElasticNet}\PY{p}{(}\PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{l1\PYZus{}ratio}\PY{o}{=}\PY{l+m+mf}{0.0000001}\PY{p}{)}
         \PY{n}{clf} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{XX\PYZus{}train}\PY{p}{,} \PY{n}{yy\PYZus{}train}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Estimating RMSLE on the test set with ElasticNet}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{score\PYZus{}kaggle\PYZus{}log}\PY{p}{(} \PY{n}{yy\PYZus{}test}\PY{p}{,} \PY{n}{clf}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{XX\PYZus{}test}\PY{p}{)} \PY{p}{)} \PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Estimating RMSLE on the test set with ElasticNet
0.651556924312904

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_124_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsection{Decisions Trees}\label{decisions-trees}

\hspace{0.5cm} Essentially, decision trees learn a hierarchy of if-else questions,
leading to a decision. In the following illustration, taken from the book
\href{http://shop.oreilly.com/product/0636920030515.do}{Introduction to
Machine Learning with Python} by \href{http://amueller.io/}{Andreas
Mueller} and \href{https://twitter.com/sarah_guido}{Sarah Guido} each
node in the tree either represents a question or a terminal node (also
called a leaf) which contains the answer.

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_126_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Decision trees may be very intuitive for classification tasks, but one
may be wondering how are the questions for continuous data, such as
ours. In this they are of the form "is feature \(i\) larger than value
\(a\)?". To build a tree, the algorithm searches over all possible
questions and find the most informative one about the target variable.
The book
\href{http://shop.oreilly.com/product/0636920030515.do}{Introduction to
Machine Learning with Python} by \href{http://amueller.io/}{Andreas
Mueller} and \href{https://twitter.com/sarah_guido}{Sarah Guido}
provides a nice visualization of this procedure.

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_128_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_128_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_128_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_128_3.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Typically, building a tree as described and continuing until all leaves
are pure leads to very complex models and highly overfit the training
data. There are to ways to prevent overfitting:

\begin{itemize}
\tightlist
\item
  Pre-pruning: stop the creation of the tree before it is ended.
\item
  Post-pruning or pruning: build the tree, but removing or collapsing
  nodes with little information.
\end{itemize}

The scikit-learn library implements only pre-pruning, on which one could
limit he maximum depth of the tree by the parameter \texttt{max\_depth}
or even limit the maximum number of leaves with
\texttt{max\_leaf\_nodes}.

As decision trees methods are widely used for classification and even
regression tasks, we will give a shot here with
\texttt{DecisionTreeRegressor}
\href{http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html}{algorithm}.
There are several important parameters such as \texttt{max\_depth},
mentioned above

As decision trees methods are widely used for classification and even
regression tasks, we will give a shot here with
\texttt{DecisionTreeRegressor}
\href{http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html}{algorithm}.
There are several important parameters such as \texttt{max\_depth}
mentioned above and \texttt{min\_samples\_split}, which controls the
minimum number of samples required to split an internal node. For now,
we will keep all parameters in their
\href{http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html}{default
values}.

    \begin{Verbatim}[commandchars=\\\{\}]
{\hspace{1.6cm}} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{tree} \PY{k}{import} \PY{n}{DecisionTreeRegressor}
         \PY{n}{model} \PY{o}{=} \PY{n}{DecisionTreeRegressor}\PY{p}{(}\PY{n}{random\PYZus{}state}\PY{o}{=} \PY{l+m+mi}{0}\PY{p}{,}\PY{n}{max\PYZus{}depth}\PY{o}{=} \PY{k+kc}{None}\PY{p}{,} \PY{n}{min\PYZus{}samples\PYZus{}split}\PY{o}{=} \PY{l+m+mi}{2}\PY{p}{)}
         \PY{n}{clf} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{XX\PYZus{}train}\PY{p}{,} \PY{n}{yy\PYZus{}train}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Estimating RMSLE on the test set with DecisionTreeRegressor}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{score\PYZus{}kaggle\PYZus{}log}\PY{p}{(} \PY{n}{yy\PYZus{}test}\PY{p}{,} \PY{n}{clf}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{XX\PYZus{}test}\PY{p}{)} \PY{p}{)} \PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Estimating RMSLE on the test set with DecisionTreeRegressor
0.661230882441467

    \end{Verbatim}

    We note \texttt{Ridge} regression is still better. As $\approx$0.66 is
quite far form $\approx$0.49, instead of try to tune the
hyperparameters of \texttt{DecisionTreeRegressor}, we decide to analyze
ensemble methods, which combine multiple models to create more powerful
models.

    \subsubsection{Ensembles of Decision
Tress}\label{ensembles-of-decision-tress}

\hspace{0.5cm} There are some models in this category, but three of them have proven to
be effective on a wide range of data set for classification and
regression and they use decision trees on the building block:
\href{https://towardsdatascience.com/the-random-forest-algorithm-d457d499ffcd}{Random
Forests},
\href{https://pt.coursera.org/learn/python-machine-learning/lecture/emwn3/gradient-boosted-decision-trees}{Gradient
Boosted Decision Trees} and
\href{http://xgboost.readthedocs.io/en/latest/}{Extreme Gradient
Boosting}.

\paragraph{Random Forests}\label{random-forests}

Random forests are essentially a collection of decision trees, where
each tree is slightly different from the others. The idea: each tree
might do a relatively good prediction job, but will likely overfit on
part of the data. If many trees are built, all of which working well and
overfitting in different ways, the amount of overfitting can be reduced
by averaging the results.


The parameter \texttt{n\_estimator} controls the number of trees in the
random forest.


Each tree is built with a
\href{http://stat.rutgers.edu/home/mxie/RCPapers/bootstrap.pdf}{bootstrap}
sample of the data. A bootstrap sample is constructed by random sampling
with replacement of the data points. Then bootstrapping creates a data
set as big as the original one, but some data points will be missing
from it and some will be repeated. Next, a decision tree is built based
on each new data set created by bootstrapping. However,in this context,
the decision tree algorithm is slightly modified. Before, it searched
for the best test for each node. In this context, it randomly selects in
each node a subset of the features and looks for the best possible test
involving one of these features. The quantity of selected features is
controlled by the \texttt{max\_features} parameter. This selection is
repeated in each node, so each of them make a decision using a different
features subset.

Note \texttt{max\_features} is a critical parameter. If
\texttt{max\_features\ =\ n\_features} no randomness is injected, while
if \texttt{max\_features\ =\ 1} the algorithm has no choice of which
feature to test and can only search for thresholds for the randomly
selected feature.

Below we aplly \texttt{RandomForestRegressor}. For now, we will keep all
parameters in their
\href{http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html}{default
values}.

    \begin{Verbatim}[commandchars=\\\{\}]
{\hspace{1.6cm}} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{ensemble} \PY{k}{import} \PY{n}{RandomForestRegressor}
         \PY{n}{model} \PY{o}{=} \PY{n}{RandomForestRegressor}\PY{p}{(}\PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
         \PY{n}{clf} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{XX\PYZus{}train}\PY{p}{,} \PY{n}{yy\PYZus{}train}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Estimating RMSLE on the test set with RandomForestRegressor}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{score\PYZus{}kaggle\PYZus{}log}\PY{p}{(} \PY{n}{yy\PYZus{}test}\PY{p}{,} \PY{n}{clf}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{XX\PYZus{}test}\PY{p}{)} \PY{p}{)} \PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Estimating RMSLE on the test set with RandomForestRegressor
0.5239454227464584

    \end{Verbatim}

    The result provided by \texttt{RandomForestRegressor} with its default
hyperparameters is already much better than the one obtained by
\texttt{DecisionTreeRegressor}. Perhaps we can get smaller score values
if we try to tune the random forest parameters using cross-validation.

    \paragraph{Gradient Boosted Regression
Trees}\label{gradient-boosted-regression-trees}

Gradient boosting models were first proposed in the paper
\href{https://statweb.stanford.edu/~jhf/ftp/trebst.pdf}{Greedy Function
Approximation: A Gradient Boosting Machine}, by
\href{https://statweb.stanford.edu/~jhf/}{Jerome H. Friedman}. Both
Gradient Boosting Regression Trees and XGBoost mentioned in this work
are based on this original proposal.

Gradient Boosted Regression Trees (or Gradient Boosted Machines) is
another ensemble method combining multiple decision trees to a more
powerful model. Despite its name, it can be used for regression and
classification.

It works by building trees in a serial manner, where each tree tries to
corrected the mistakes of the previous one. There is no randomization.
Instead, strong pre-pruning is used. It often uses very shallow trees,
of depth one to five, making a faster model with less memory use.

The main idea of gradient boosted: combine many simple models (known as
weak learners) like shallow trees. Each tree can provide good
predictions only on part of the data, and more and more trees are added
to iteratively improve performance.

They are generally a bit more sensitive to parameter settings than
random forests, but can provide better accuracy if the parameters are
set correctly (pre-pruning, number of trees in the ensemble and learning
rate are the model most important features).

The \texttt{learning\_rate} parameter controls how strongly each tree
tries to correct the mistakes of the previous ones. High
\texttt{learning\_rate} means each tree make strong corrections,
allowing for a more complex model. Similarly, adding more trees to the
ensemble, \texttt{n\_estimators} also increases model complexity.

Below we aplly \texttt{GradientBoostingRegressor}. For now, we will keep
all parameters in their
\href{http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html}{default
values}.

    \begin{Verbatim}[commandchars=\\\{\}]
{\hspace{1.6cm}} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{ensemble} \PY{k}{import} \PY{n}{GradientBoostingRegressor}
         \PY{n}{model} \PY{o}{=} \PY{n}{GradientBoostingRegressor}\PY{p}{(}\PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
         \PY{n}{clf} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{XX\PYZus{}train}\PY{p}{,} \PY{n}{yy\PYZus{}train}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Estimating RMSLE on the test set with GradientBoostingRegressor}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{score\PYZus{}kaggle\PYZus{}log}\PY{p}{(} \PY{n}{yy\PYZus{}test}\PY{p}{,} \PY{n}{clf}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{XX\PYZus{}test}\PY{p}{)} \PY{p}{)} \PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Estimating RMSLE on the test set with GradientBoostingRegressor
0.471361235487172

    \end{Verbatim}

    Without changing any of the hyperparameters default values,
\texttt{GradientBoostingRegressor} achieved the lowest score value so
far. Certainly, we will do cross-validation with this algorithm.

    \paragraph{Extreme Gradient Boosting:
XGBoost}\label{extreme-gradient-boosting-xgboost}

\href{https://arxiv.org/pdf/1603.02754.pdf}{XGBoost} is an optimized
distributed gradient boosting
\href{https://github.com/dmlc/xgboost}{library} designed to be highly
efficient, flexible and portable. It implements machine learning
algorithms under the
\href{http://xgboost.readthedocs.io/en/latest/model.html}{Gradient
Boosting framework}.
\href{http://xgboost.readthedocs.io/en/latest/}{XGBoost} provides a
parallel tree boosting (also known as GBDT, GBM) that solve many data
science problems in a fast and accurate way. The same code runs on major
distributed environment (\href{https://hadoop.apache.org/}{Hadoop}, SGE,
MPI) and can solve problems beyond billions of examples.

The \texttt{xgboost} package is worth looking to apply gradient boosting
to a large scale problem, because it is usually faster and sometimes
easier to tune than the \href{http://scikit-learn.org/}{scikit-learn}
implementation of gradient boosting on many data sets. Below we first
apply \texttt{XGBRegressor} with its parameters set in their
\href{http://xgboost.readthedocs.io/en/latest/parameter.html}{default
values}.

    \begin{Verbatim}[commandchars=\\\{\}]
{\hspace{1.6cm}} \PY{k+kn}{import} \PY{n+nn}{xgboost} \PY{k}{as} \PY{n+nn}{xgb}
         \PY{n}{model} \PY{o}{=} \PY{n}{xgb}\PY{o}{.}\PY{n}{XGBRegressor}\PY{p}{(}\PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
         \PY{n}{clf} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{XX\PYZus{}train}\PY{p}{,} \PY{n}{yy\PYZus{}train}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Estimating RMSLE on the test set with XGBRegressor}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{score\PYZus{}kaggle\PYZus{}log}\PY{p}{(} \PY{n}{yy\PYZus{}test}\PY{p}{,} \PY{n}{clf}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{XX\PYZus{}test}\PY{p}{)} \PY{p}{)} \PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Estimating RMSLE on the test set with XGBRegressor
0.47136019069697366

    \end{Verbatim}

    The performance of \texttt{XGBRegressor} is almost the same of
\texttt{GradientBoostingRegressor}, which indicates it is worth tuning
the hyperparameters of these two models. Since \texttt{Ridge} did not do
a bad job either, we will work on tuning its parameters as well.

    \subsection{Cross-validation and Grid
Search}\label{cross-validation-and-grid-search}

\hspace{0.5cm} \href{https://en.wikipedia.org/wiki/Cross-validation_(statistics)}{Cross-validation}
is a model validation technique for assessing how the results of a
statistical analysis will generalize to an independent data set,
preventing overfitting. Note that cross-validation does not measure
generalization, it just tries to estimate from the training set itself
how well the model will generalize.

In cross-validation, the same model is run several times using a
fraction of the training set and storing what is left as a validation
set, which is used to evaluated the performance of the model. This is
procedure is run several times, until all the training data have played
the part of a validation set. The cross-validation resulting model is
then the average of the ones computed in the loop.

The figure below, taken from this
\href{https://www.kaggle.com/dansbecker/cross-validation/code}{tutorial}
by \href{https://www.kaggle.com/dansbecker}{DanB} illustrates the
procedure for 5 folds.
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_142_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    

    Additionally to cross-validation, we use
\href{https://en.wikipedia.org/wiki/Hyperparameter_optimization}{grid-search}
to estimate the best hyperparameters to generalization. It is simply an
exhaustive searching through a manually specified subset of the
hyperparameter space of a learning algorithm. A grid search algorithm
must be guided by some performance metric, typically measured by
cross-validation on the training set or evaluation on a held-out
validation set.

For example, let us suppose a machine learning model has at least two
hyperparameters that need to be tuned for good performance on unseen
data, namely $A$ and $B$. To perform grid search, suppose one
selects a finite set of "reasonable" values for each, say $A \in \{
a_1 , a_2 , a_3 \} $ and $ b \in \{ b_1 , b_2 , b_3 \} $. Grid
search then trains the model with each pair $ (A,B) $ in the Cartesian
product of these two sets and evaluates their performance on a held-out
validation set (or by internal cross-validation on the training set).
Finally,the grid search algorithm outputs the settings that achieved the
highest score in the validation procedure.

In \href{http://scikit-learn.org/stable/index.html}{scikit-learn}
grid-search and cross-validation are applied
\href{http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html}{simultaneously}
with \texttt{GridSearchCV}.

    \subsubsection{Cross-validation and Grid-search with Ridge
Regression}\label{cross-validation-and-grid-search-with-ridge-regression}

\hspace{0.5cm} First we perform cross-validation with the Ridge Regression by changing
the hyperparameter \texttt{alpha}, which controls the regularization
strength (default=1.0). We also investigate the performance for
different choices of \texttt{solver}:

\begin{itemize}
\tightlist
\item
  \texttt{auto}: chooses the solver automatically based on the type of
  data.
\item
  \texttt{svd}: uses a Singular Value Decomposition of X to compute the
  Ridge coefficients.
\item
  \texttt{lsqr}: uses the dedicated regularized least-squares routine
  scipy.sparse.linalg.lsqr. It is the fastest but may not be available
  in old scipy versions. It also uses an iterative procedure.
\item
  \texttt{sag}: uses a Stochastic Average Gradient descent.
\end{itemize}

    \begin{Verbatim}[commandchars=\\\{\}]
{\hspace{1.6cm}} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{linear\PYZus{}model} \PY{k}{import} \PY{n}{Ridge}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k}{import} \PY{n}{GridSearchCV}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k}{import} \PY{n}{make\PYZus{}scorer}
         \PY{c+c1}{\PYZsh{} greater\PYZus{}is\PYZus{}better= False: we want to minimize the root mean square logarithmic error}
         \PY{n}{scorer} \PY{o}{=} \PY{n}{make\PYZus{}scorer}\PY{p}{(}\PY{n}{score\PYZus{}kaggle\PYZus{}log}\PY{p}{,} \PY{n}{greater\PYZus{}is\PYZus{}better}\PY{o}{=} \PY{k+kc}{False}\PY{p}{)}
         \PY{n}{model} \PY{o}{=} \PY{n}{Ridge}\PY{p}{(}\PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
         \PY{n}{parameters}\PY{o}{=}\PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{alpha}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mf}{0.01}\PY{p}{,} \PY{l+m+mf}{0.1}\PY{p}{,} \PY{l+m+mf}{1.0}\PY{p}{,} \PY{l+m+mf}{10.0}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{solver}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{auto}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{lsqr}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sag}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{svd}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{\PYZcb{}}
         \PY{n}{clf} \PY{o}{=} \PY{n}{GridSearchCV}\PY{p}{(}\PY{n}{model}\PY{p}{,} \PY{n}{param\PYZus{}grid}\PY{o}{=} \PY{n}{parameters}\PY{p}{,} \PY{n}{scoring}\PY{o}{=} \PY{n}{scorer}\PY{p}{,}  \PY{n}{cv}\PY{o}{=} \PY{l+m+mi}{3}\PY{p}{)}
         \PY{n}{grid\PYZus{}fit} \PY{o}{=} \PY{n}{clf}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{XX\PYZus{}train}\PY{p}{,} \PY{n}{yy\PYZus{}train}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{grid\PYZus{}fit}\PY{o}{.}\PY{n}{best\PYZus{}params\PYZus{}}\PY{p}{)}
         \PY{n}{best\PYZus{}clf} \PY{o}{=} \PY{n}{grid\PYZus{}fit}\PY{o}{.}\PY{n}{best\PYZus{}estimator\PYZus{}}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{score\PYZus{}kaggle\PYZus{}log}\PY{p}{(} \PY{n}{yy\PYZus{}test}\PY{p}{,} \PY{n}{best\PYZus{}clf}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{XX\PYZus{}test}\PY{p}{)} \PY{p}{)} \PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Fitting 3 folds for each of 16 candidates, totalling 48 fits
    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
[Parallel(n\_jobs=-1)]: Done  48 out of  48 | elapsed:  1.7min finished
    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
\{'alpha': 0.1, 'solver': 'auto'\}
0.492698026491787
    \end{Verbatim}

    The cross-validation procedure suggests we should have set
\texttt{alpha=0.1} instead of \texttt{alpha=1.0} (0.4926978071042146) in
order to improve the performance based on the root mean square
logarithmic error. The gain is quite small, but in a Kaggle competition
could represent several positions on the leaderboard.

    \subsubsection{Cross-validation and Grid-search with Random Forest
Regression}\label{cross-validation-and-grid-search-with-random-forest-regression}

\hspace{0.5cm} Now we check the behavior of \texttt{RandomForestRegressor} by changing
the following hyperparameters:

\begin{itemize}
\tightlist
\item
  \texttt{max\_depth} : the maximum depth of the tree
  (default=\texttt{None}). If \texttt{None}, then nodes are expanded
  until all leaves are pure or until all leaves contain less than
  \texttt{min\_samples\_split} samples.
\item
  \texttt{n\_estimators} : the number of trees in the forest
  (default=10).
\item
  \texttt{min\_samples\_split}: the minimum number of samples required
  to split an internal node (default=2)
\item
  \texttt{max\_features} : the number of features to consider when
  looking for the best split (default=\texttt{auto}). If \texttt{auto},
  then \texttt{max\_features=n\_features}, if \texttt{sqrt}, then
  \texttt{max\_features=sqrt(n\_features)}.
\end{itemize}

    \begin{Verbatim}[commandchars=\\\{\}]
{\hspace{1.6cm}} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{ensemble} \PY{k}{import} \PY{n}{RandomForestRegressor}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k}{import} \PY{n}{GridSearchCV}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k}{import} \PY{n}{make\PYZus{}scorer}
         \PY{c+c1}{\PYZsh{} greater\PYZus{}is\PYZus{}better= False: we want to minimize the root mean square logarithmic error}
         \PY{n}{scorer} \PY{o}{=} \PY{n}{make\PYZus{}scorer}\PY{p}{(}\PY{n}{score\PYZus{}kaggle\PYZus{}log}\PY{p}{,} \PY{n}{greater\PYZus{}is\PYZus{}better}\PY{o}{=} \PY{k+kc}{False}\PY{p}{)}
         \PY{n}{model} \PY{o}{=} \PY{n}{RandomForestRegressor}\PY{p}{(}\PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
         \PY{n}{parameters} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{max\PYZus{}depth}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{15}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{n\PYZus{}estimators}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mi}{20}\PY{p}{,} \PY{l+m+mi}{30}\PY{p}{]}\PY{p}{,}
                       \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{min\PYZus{}samples\PYZus{}split}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mi}{6}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{max\PYZus{}features}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{auto}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sqrt}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{\PYZcb{}}
         \PY{n}{clf} \PY{o}{=} \PY{n}{GridSearchCV}\PY{p}{(}\PY{n}{model}\PY{p}{,} \PY{n}{param\PYZus{}grid}\PY{o}{=} \PY{n}{parameters}\PY{p}{,} \PY{n}{scoring}\PY{o}{=} \PY{n}{scorer}\PY{p}{,} \PY{n}{verbose}\PY{o}{=} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{cv}\PY{o}{=} \PY{l+m+mi}{2}\PY{p}{)}
         \PY{n}{grid\PYZus{}fit} \PY{o}{=} \PY{n}{clf}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{XX\PYZus{}train}\PY{p}{,} \PY{n}{yy\PYZus{}train}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{grid\PYZus{}fit}\PY{o}{.}\PY{n}{best\PYZus{}params\PYZus{}}\PY{p}{)}
         \PY{n}{best\PYZus{}clf} \PY{o}{=} \PY{n}{grid\PYZus{}fit}\PY{o}{.}\PY{n}{best\PYZus{}estimator\PYZus{}}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{score\PYZus{}kaggle\PYZus{}log}\PY{p}{(} \PY{n}{yy\PYZus{}test}\PY{p}{,} \PY{n}{best\PYZus{}clf}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{XX\PYZus{}test}\PY{p}{)} \PY{p}{)} \PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Fitting 2 folds for each of 16 candidates, totalling 32 fits
    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
[Parallel(n\_jobs=1)]: Done  32 out of  32 | elapsed: 19.3min finished
    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
\{'max\_depth': 15, 'max\_features': 'auto', 'min\_samples\_split': 10, 'n\_estimators': 30\}
0.4724815086016824
    \end{Verbatim}

    Varying three hyperparameters considerably improved the previous score
(0.5239454227464584). Although cross-validation has indicated we should
keep \texttt{max\_features\ =\ n\_features} (no randomness is injected),
the other tree hyperparameters should have been changed: the maximum
depth of tree increase limited to 15, the minimum number of samples
required to split an internal node raised from 2 to 10 (more complexity)
and the number of trees in the forest increased from 10 to 20. The RMSLE
score for the fictitious test set obtained by the random forest
algorithm with these hyperparameters is the best we have obtained so
far.

\subsubsection{Cross-validation and Grid-search with Gradient Boosting
Regression}\label{cross-validation-and-grid-search-with-gradient-boosting-regression}

\hspace{0.5cm} Now we check the behavior of \texttt{GradientBoostingRegressor} by
changing the following hyperparameters:

\begin{itemize}
\tightlist
\item
  \texttt{max\_depth} : maximum depth of the individual regression
  estimators (default=3). The maximum depth limits the number of nodes
  in the tree. The best value depends on the interaction of the input
  variables.
\item
  \texttt{n\_estimators} : the number of boosting stages to perform
  (default=100). Gradient boosting is fairly robust to overfitting so a
  large number usually results in better performance.
\item
  \texttt{min\_samples\_split}: the minimum number of samples required
  to split an internal node (default=2)
\item
  \texttt{leanirng\_rate}: shrinks the contribution of each tree
  (default0.1). There is a trade-off between \texttt{learning\_rate} and
  \texttt{n\_estimators}.
\end{itemize}

    \begin{Verbatim}[commandchars=\\\{\}]
{\hspace{1.6cm}} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{ensemble} \PY{k}{import} \PY{n}{GradientBoostingRegressor}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k}{import} \PY{n}{GridSearchCV}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k}{import} \PY{n}{make\PYZus{}scorer}
         \PY{c+c1}{\PYZsh{} greater\PYZus{}is\PYZus{}better= False: we want to minimize the root mean square logarithmic error}
         \PY{n}{scorer} \PY{o}{=} \PY{n}{make\PYZus{}scorer}\PY{p}{(}\PY{n}{score\PYZus{}kaggle\PYZus{}log}\PY{p}{,} \PY{n}{greater\PYZus{}is\PYZus{}better}\PY{o}{=} \PY{k+kc}{False}\PY{p}{)}
         \PY{n}{model} \PY{o}{=} \PY{n}{GradientBoostingRegressor}\PY{p}{(}\PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
         \PY{n}{parameters} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{max\PYZus{}depth}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{n\PYZus{}estimators}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mi}{100}\PY{p}{,} \PY{l+m+mi}{200}\PY{p}{]}\PY{p}{,}
                       \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{min\PYZus{}samples\PYZus{}split}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{6}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{learning\PYZus{}rate}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mf}{0.1}\PY{p}{,} \PY{l+m+mf}{1.0}\PY{p}{]}\PY{p}{\PYZcb{}}
         \PY{n}{clf} \PY{o}{=} \PY{n}{GridSearchCV}\PY{p}{(}\PY{n}{model}\PY{p}{,} \PY{n}{param\PYZus{}grid}\PY{o}{=} \PY{n}{parameters}\PY{p}{,} \PY{n}{scoring}\PY{o}{=} \PY{n}{scorer}\PY{p}{,} \PY{n}{verbose}\PY{o}{=} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{cv}\PY{o}{=} \PY{l+m+mi}{2}\PY{p}{)}
         \PY{n}{grid\PYZus{}fit} \PY{o}{=} \PY{n}{clf}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{XX\PYZus{}train}\PY{p}{,} \PY{n}{yy\PYZus{}train}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{grid\PYZus{}fit}\PY{o}{.}\PY{n}{best\PYZus{}params\PYZus{}}\PY{p}{)}
         \PY{n}{best\PYZus{}clf} \PY{o}{=} \PY{n}{grid\PYZus{}fit}\PY{o}{.}\PY{n}{best\PYZus{}estimator\PYZus{}}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{score\PYZus{}kaggle\PYZus{}log}\PY{p}{(} \PY{n}{yy\PYZus{}test}\PY{p}{,} \PY{n}{best\PYZus{}clf}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{XX\PYZus{}test}\PY{p}{)} \PY{p}{)} \PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Fitting 2 folds for each of 16 candidates, totalling 32 fits
    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
[Parallel(n\_jobs=1)]: Done  32 out of  32 | elapsed: 86.5min finished
    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
\{'learning\_rate': 0.1, 'max\_depth': 5, 'min\_samples\_split': 2, 'n\_estimators': 200\}
0.46219359665138765
    \end{Verbatim}

    Varying two hyperparameters slightly improved ($\approx$1.95\%) the
previous score (0.471361235487172). As mentioned before, such a gain
could not appear to be worthwhile, especially if we compare the training
times so far (note \texttt{GradientBoostingRegressor} took much more
time to train). Yet 2\% in a competition could represent several
positions in the leaderboard.

Although cross-validation has indicated we should keep
\texttt{learning\_rate=0.1} and \texttt{min\_samples\_split=2}, the
other two hyperparameters should have been changed: the maximum depth of
the individual regression estimators (limits the number of nodes in the
tree) increased from 3 to 5 and the number of boosting stages to perform
from 100 to 200. The RMSLE score for the fictitious test set obtained by
the gradient boosting algorithm with these hyperparameters is the best
we have obtained so far.

    \subsubsection{Cross-validation and Grid-search with XGBoost
Regression}\label{cross-validation-and-grid-search-with-xgboost-regression}

\hspace{0.5cm} Now we check the behavior of \texttt{XGBRegressor} by changing the
following
\href{http://xgboost.readthedocs.io/en/latest/python/python_api.html}{hyperparameters}:

\begin{itemize}
\tightlist
\item
  \texttt{max\_depth} : maximum depth of a tree (default=3), increase
  this value will make the model more complex/likely to be overfitting.
\item
  \texttt{n\_estimators} : number of boosted trees to fit (default=100).
\item
  \texttt{leanirng\_rate}: shrinks the contribution of each tree
  (default=0.1).
\item
  \texttt{min\_samples\_split}: the minimum number of samples required
  to split an internal node (default=2)
\item
  \texttt{reg\_lambda}: L2 regularization term on weights (default=1.0).
\end{itemize}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}84}]:} \PY{k+kn}{import} \PY{n+nn}{xgboost} \PY{k}{as} \PY{n+nn}{xgb}
          \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k}{import} \PY{n}{GridSearchCV}
          \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k}{import} \PY{n}{make\PYZus{}scorer}
          \PY{c+c1}{\PYZsh{} greater\PYZus{}is\PYZus{}better= False: we want to minimize the root mean square logarithmic error}
          \PY{n}{scorer} \PY{o}{=} \PY{n}{make\PYZus{}scorer}\PY{p}{(}\PY{n}{score\PYZus{}kaggle\PYZus{}log}\PY{p}{,} \PY{n}{greater\PYZus{}is\PYZus{}better}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
          \PY{n}{model} \PY{o}{=} \PY{n}{xgb}\PY{o}{.}\PY{n}{XGBRegressor}\PY{p}{(}\PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
          \PY{n}{parameters} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{max\PYZus{}depth}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{20}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{n\PYZus{}estimators}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mi}{100}\PY{p}{,} \PY{l+m+mi}{200}\PY{p}{]}\PY{p}{,}
                        \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{learning\PYZus{}rate}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mf}{0.1}\PY{p}{,} \PY{l+m+mf}{1.0}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{reg\PYZus{}lambda}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mf}{0.1}\PY{p}{,} \PY{l+m+mf}{1.0}\PY{p}{]} \PY{p}{\PYZcb{}}
          \PY{n}{clf} \PY{o}{=} \PY{n}{GridSearchCV}\PY{p}{(}\PY{n}{model}\PY{p}{,} \PY{n}{param\PYZus{}grid}\PY{o}{=} \PY{n}{parameters}\PY{p}{,} \PY{n}{scoring}\PY{o}{=} \PY{n}{scorer}\PY{p}{,} \PY{n}{cv}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}
          \PY{n}{grid\PYZus{}fit} \PY{o}{=} \PY{n}{clf}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{XX\PYZus{}train}\PY{p}{,} \PY{n}{yy\PYZus{}train}\PY{p}{)}
          \PY{n+nb}{print}\PY{p}{(}\PY{n}{grid\PYZus{}fit}\PY{o}{.}\PY{n}{best\PYZus{}params\PYZus{}}\PY{p}{)}
          \PY{n}{best\PYZus{}clf} \PY{o}{=} \PY{n}{grid\PYZus{}fit}\PY{o}{.}\PY{n}{best\PYZus{}estimator\PYZus{}}
          \PY{n+nb}{print}\PY{p}{(}\PY{n}{score\PYZus{}kaggle\PYZus{}log}\PY{p}{(} \PY{n}{yy\PYZus{}test}\PY{p}{,} \PY{n}{best\PYZus{}clf}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{XX\PYZus{}test}\PY{p}{)} \PY{p}{)} \PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Fitting 2 folds for each of 24 candidates, totalling 48 fits
    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
[Parallel(n\_jobs=-1)]: Done  48 out of  48 | elapsed: 112.6min finished
    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
\{'learning\_rate': 0.1, 'max\_depth': 5, 'n\_estimators': 200, 'reg\_lambda': 1.0\}
0.46214027536016206
    \end{Verbatim}

    Juke like \texttt{GradientBoostingRegression}, the variation is
suggested only on maximum depth (from 3 to 5) and number of estimators
(from 100 to 200). The performance is also very similar
($\approx$0.01\% in favor of \texttt{XGBRegressor}). The major
advantage in favor to extreme boosting in this case is the training
time. While each fit using the usual gradient boosting took, on average,
2.7 minutes, extreme boosting took 2.35 minutes (on average) to complete
each fit. This would represent more than an hour if we had more than 170
fits.

\hspace{0.5cm} \section{Training with full training data using the cross-validation
hyperparameters and submitting to
Kaggle}\label{training-with-full-training-data-using-the-cross-validation-hyperparameters-and-submitting-to-kaggle}

\hspace{0.5cm} Although up to now boosting methods have led to better results, we still
have not trained any algorithm with full training data in order to make
predictions on the real test set. Since ridge regression and random
forests are quite fast to train and did not lead to terrible estimators,
we are still using them in this final section.

As mentioned before, performance of predictions on the real test set
cannot be evaluated here. The final score is achieved only when a
submission to Kaggle is made.

\subsection{Benchmark}\label{benchmark}

\hspace{0.5cm} The winner team, composed by
\href{https://www.kaggle.com/lapalma}{Francesco Palma},
\href{https://www.kaggle.com/aldopod}{Aldo Podestá},
\href{https://www.kaggle.com/tomboys}{TomBoy} and
\href{https://www.kaggle.com/wallyoliveira}{W.R. Lemes de Oliveira}
obtained the score 0.28976 on the private
\href{https://www.kaggle.com/c/nyc-taxi-trip-duration/leaderboard}{leaderboard},
which is calculated with approximately 70\% of the test data. Naturally,
their solution is the benchmark model. However, it seems clear to us
that, to begin with, much more sophisticated data preprocessing and
future extraction, perhaps using clusters and principal component
analysis, should be carried on in order to achieve some score close to
theirs.

Since our purpose is learning, not competing, we define our benchmark as
the score obtained on the private learderboard with the most naive and
straightforward estimation we could make: the mean value on the training
set.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}} \PY{n}{mean\PYZus{}naive\PYZus{}trip\PYZus{}duration} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{exp}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{p}{)}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{l+m+mi}{1}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}} 645.8218576931354
\end{Verbatim}
             
    The submission file constructed with the most naive estimation we
could make, the mean value of \texttt{trip\_duration} in the training
set, achieved the benchmark score 0.79807 on private
\href{https://www.kaggle.com/c/nyc-taxi-trip-duration/leaderboard}{leaderboard}.

\subsection{Ridge Regression}\label{ridge-regression}

\hspace{0.5cm} We start by training a Ridge regressor with regularization strength
\texttt{alpha} equals to 0.1, following the cross-validation analysis.

    \begin{Verbatim}[commandchars=\\\{\}]
{\hspace{1.6cm}} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{linear\PYZus{}model} \PY{k}{import} \PY{n}{Ridge}       
         \PY{n}{model} \PY{o}{=} \PY{n}{Ridge}\PY{p}{(}\PY{n}{random\PYZus{}state}\PY{o}{=} \PY{l+m+mi}{0}\PY{p}{,} \PY{n}{alpha}\PY{o}{=} \PY{l+m+mf}{0.1}\PY{p}{)}         
         \PY{n}{clf} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
         \PY{n}{test\PYZus{}data\PYZus{}pred} \PY{o}{=}  \PY{n}{np}\PY{o}{.}\PY{n}{exp}\PY{p}{(} \PY{n}{clf}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{test\PYZus{}data}\PY{p}{)}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{l+m+mi}{1} 
\end{Verbatim}
          
    The submission file constructed with Ridge regression achieved
the score 0.51054 on private
\href{https://www.kaggle.com/c/nyc-taxi-trip-duration/leaderboard}{leaderboard}, which is already much better than the benchmark.

    \subsection{Random Forest Regression}\label{random-forest-regression}

\hspace{0.5cm} We now train using random forest regression with \texttt{max\_depth=15},
\texttt{min\_samples\_split=\ 10} and \texttt{n\_estimators=30},
following the cross-validation analysis.

    \begin{Verbatim}[commandchars=\\\{\}]
{\hspace{1.6cm}} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{ensemble} \PY{k}{import} \PY{n}{RandomForestRegressor}
         \PY{n}{model} \PY{o}{=} \PY{n}{RandomForestRegressor}\PY{p}{(}\PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{max\PYZus{}depth}\PY{o}{=}\PY{l+m+mi}{15}\PY{p}{,} \PY{n}{min\PYZus{}samples\PYZus{}split}\PY{o}{=} \PY{l+m+mi}{10}\PY{p}{,} 
         		    	  \PY{n}{n\PYZus{}estimators}\PY{o}{=}\PY{l+m+mi}{30}\PY{p}{)}        
         \PY{n}{clf} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
         \PY{n}{test\PYZus{}data\PYZus{}pred} \PY{o}{=}  \PY{n}{np}\PY{o}{.}\PY{n}{exp}\PY{p}{(} \PY{n}{clf}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{test\PYZus{}data}\PY{p}{)}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{l+m+mi}{1} 
\end{Verbatim}

    The submission file constructed with Random Forest regression
achieved the score 0.51063 on private
\href{https://www.kaggle.com/c/nyc-taxi-trip-duration/leaderboard}{leaderboard}.
Ridge regression, though its simplicity, is still better.

    \subsection{Extreme Gradient
Boosting}\label{extreme-gradient-boosting}

\hspace{0.5cm} We now train using extreme gradient boosting with
\texttt{learning\_rate=0.1}, \texttt{max\_depth=5},
\texttt{n\_estimators=200} following the cross-validation analysis.

    \begin{Verbatim}[commandchars=\\\{\}]
{\hspace{1.6cm}} \PY{k+kn}{import} \PY{n+nn}{xgboost} \PY{k}{as} \PY{n+nn}{xgb}
          \PY{n}{model} \PY{o}{=} \PY{n}{xgb}\PY{o}{.}\PY{n}{XGBRegressor}\PY{p}{(}\PY{n}{random\PYZus{}state}\PY{o}{=} \PY{l+m+mi}{0}\PY{p}{,} \PY{n}{learning\PYZus{}rate}\PY{o}{=} \PY{l+m+mf}{0.1}\PY{p}{,} \PY{n}{max\PYZus{}depth}\PY{o}{=} \PY{l+m+mi}{5}\PY{p}{,}
                                   \PY{n}{n\PYZus{}estimators}\PY{o}{=} \PY{l+m+mi}{200}\PY{p}{,} \PY{n}{reg\PYZus{}lambda}\PY{o}{=} \PY{l+m+mf}{1.0}\PY{p}{)} 
          \PY{n}{clf} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
          \PY{n}{test\PYZus{}data\PYZus{}pred} \PY{o}{=}  \PY{n}{np}\PY{o}{.}\PY{n}{exp}\PY{p}{(} \PY{n}{clf}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{test\PYZus{}data}\PY{p}{)}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{l+m+mi}{1} 
\end{Verbatim}
            
    The submission file constructed with Extreme Gradient Boosting
achieved the score 0.49961 on private
\href{https://www.kaggle.com/c/nyc-taxi-trip-duration/leaderboard}{leaderboard}
and this result fulfills our needs for now, since we have defined our
goal as a score less than 0.5.

    \subsection{Gradient Boosting}\label{gradient-boosting}

\hspace{0.5cm} Finishing, we now train using gradient boosting with
\texttt{learning\_rate=0.1}, \texttt{max\_depth=5},
\texttt{min\_samples\_split=2}, \texttt{n\_estimators=200} following the
cross-validation analysis.

    \begin{Verbatim}[commandchars=\\\{\}]
{\hspace{1.6cm}} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{ensemble} \PY{k}{import} \PY{n}{GradientBoostingRegressor}
          \PY{n}{model} \PY{o}{=} \PY{n}{GradientBoostingRegressor}\PY{p}{(}\PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{max\PYZus{}depth}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{,}\PY{n}{min\PYZus{}samples\PYZus{}split}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} 
                                            \PY{n}{n\PYZus{}estimators}\PY{o}{=}\PY{l+m+mi}{200}\PY{p}{,} \PY{n}{learning\PYZus{}rate}\PY{o}{=}\PY{l+m+mf}{0.1}\PY{p}{)} 
          \PY{n}{clf} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
          \PY{n}{test\PYZus{}data\PYZus{}pred} \PY{o}{=}  \PY{n}{np}\PY{o}{.}\PY{n}{exp}\PY{p}{(} \PY{n}{clf}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{test\PYZus{}data}\PY{p}{)}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{l+m+mi}{1} 
\end{Verbatim}
            
    The submission file above, constructed with the usual Gradient Boosting
regression achieved the score 0.49960 on private
\href{https://www.kaggle.com/c/nyc-taxi-trip-duration/leaderboard}{leaderboard}.
Similarly to our estimation analysis, it is almost the same result as
the one obtained using extreme boosting.

\section{Model Evaluation and Validation of the Final
Model}\label{model-evaluation-and-validation-of-the-final-model}

\subsection{Model Evaluation}

\hspace{0.5cm} Since the best score was achieved by \texttt{GradientBoostingRegressor},
this will be taken as our final model. In order to validate the
robustness of the model's solution we run the model with multiple
different random states and analyze the mean and the standard deviation
of the predicted values for \texttt{test\_duration}.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{ensemble} \PY{k}{import} \PY{n}{GradientBoostingRegressor}
         \PY{n}{mean\PYZus{}list} \PY{o}{=} \PY{p}{[}\PY{p}{]}
         \PY{n}{std\PYZus{}list} \PY{o}{=} \PY{p}{[}\PY{p}{]}
         \PY{n}{random\PYZus{}list} \PY{o}{=} \PY{p}{[}\PY{p}{]}
         \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{)}\PY{p}{:}
             \PY{n}{random\PYZus{}state} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{randint}\PY{p}{(}\PY{n}{low}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{high}\PY{o}{=}\PY{l+m+mi}{100000}\PY{p}{)}
             \PY{n}{model} \PY{o}{=} \PY{n}{GradientBoostingRegressor}\PY{p}{(}\PY{n}{random\PYZus{}state}\PY{o}{=}\PY{n}{random\PYZus{}state}\PY{p}{,} \PY{n}{max\PYZus{}depth}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{,}
                                               \PY{n}{min\PYZus{}samples\PYZus{}split}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{n\PYZus{}estimators}\PY{o}{=}\PY{l+m+mi}{200}\PY{p}{,} 
                                               \PY{n}{learning\PYZus{}rate}\PY{o}{=}\PY{l+m+mf}{0.1}\PY{p}{)} 
             \PY{n}{clf} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
             \PY{n}{test\PYZus{}data\PYZus{}pred} \PY{o}{=}  \PY{n}{np}\PY{o}{.}\PY{n}{exp}\PY{p}{(} \PY{n}{clf}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{test\PYZus{}data}\PY{p}{)}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{l+m+mi}{1}
             \PY{n}{mean\PYZus{}list}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{test\PYZus{}data\PYZus{}pred}\PY{p}{)}\PY{p}{)}
             \PY{n}{std\PYZus{}list}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{std}\PY{p}{(}\PY{n}{test\PYZus{}data\PYZus{}pred}\PY{p}{)}\PY{p}{)}
             \PY{n}{random\PYZus{}list}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{random\PYZus{}state}\PY{p}{)} 
\end{Verbatim}


    We can check below that the model's dependence with the random number
generator seed is negligible, which indicates a reasonable degree of
robustness.


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_188_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}} \PY{n+nb}{print}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{min}\PY{p}{(}\PY{n}{mean\PYZus{}list}\PY{p}{)}\PY{p}{)}
          \PY{n+nb}{print}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{n}{mean\PYZus{}list}\PY{p}{)}\PY{p}{)}
          \PY{n}{np}\PY{o}{.}\PY{n}{min}\PY{p}{(}\PY{n}{mean\PYZus{}list}\PY{p}{)} \PY{o}{/} \PY{n}{np}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{n}{mean\PYZus{}list}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
781.7650576403955
781.7768907607054
0.9999848638141524
    \end{Verbatim}

            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}\hspace{1.8cm}} \PY{n+nb}{print}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{min}\PY{p}{(}\PY{n}{std\PYZus{}list}\PY{p}{)}\PY{p}{)}
          \PY{n+nb}{print}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{n}{std\PYZus{}list}\PY{p}{)}\PY{p}{)}
          \PY{n}{np}\PY{o}{.}\PY{n}{min}\PY{p}{(}\PY{n}{std\PYZus{}list}\PY{p}{)} \PY{o}{/} \PY{n}{np}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{n}{std\PYZus{}list}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
535.9442982005273
535.9871388111197
0.9999200715698376
    \end{Verbatim}
            
    \subsection{Justification}\label{justification}

\hspace{0.5cm} Besides being robust, the performance of the model constructed with
\texttt{GradientBoostingRegressor} was considerable better (0.49960 on
private
\href{https://www.kaggle.com/c/nyc-taxi-trip-duration/leaderboard}{leaderboard})
than our naive benchmark (0.79807 on private
\href{https://www.kaggle.com/c/nyc-taxi-trip-duration/leaderboard}{leaderboard}).

\section{Conclusions}\label{conclusions}

\subsection{Free-Form Visualization}\label{free-form-visualization}

\hspace{0.5cm} It is interesting to visualize the attribute
\texttt{feature\_importances\_} from
\href{http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html}{\texttt{GradientBoostingRegressor}}.
The higher the feature importance value, the more important the feature.


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_192_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Clearly, as it should be reasonable to guess, the most important feature
is travel distance. Weekends (take a look at
\texttt{week\_day\_Saturday} and \texttt{week\_day\_Sunday}), as we have
discussed before, also play an important role, just like some values of
pickup hour.

    \subsection{Reflection}\label{reflection}

\hspace{0.5cm} In the first steps of the project, as in almost all machine leaning
applications, we have conducted data visualization and data
preprocessing. Numerical skewed data was log-transformed and some
meaningless examples, based on an defined distance, were carefully
neglected. After that, a visual investigation was carried to extract at
least new insights about the remaining features.

In the training part, after running simple linear models, we chose to
investigate in detail the application of ensemble methods, namely:
random forest and gradient boosting. Since we did not have directly
access to the train targets, we have constructed a fictitious test set
with test targets using part of the training data. Then, by making use
of the conclusion obtained with grid-search and cross-validation we have
found \texttt{GradientBoostingRegressor} as the best algorithm choice to
be applied to all training data. It achieved score 0.49960 on private
\href{https://www.kaggle.com/c/nyc-taxi-trip-duration/leaderboard}{leaderboard}),
which represented an considerable improvement to our naive benchmark
(0.79807 on private
\href{https://www.kaggle.com/c/nyc-taxi-trip-duration/leaderboard}{leaderboard}).

To finish, the model was validated by the analysis of mean and standard
deviation of multiple models run with different random generator seed.
Also, visualization of feature importance has showed that travel
distance is the most relevant feature in the dataset. Perhaps this
indicates we have spent too much time dealing with features other than
latitude and longitude, when in fact these are the ones we should had
taken a more careful look.

\subsection{Improvement}\label{improvement}

\hspace{0.5cm} Since we have built multiple models, a possible improvement would be to
'ensemble' all of them. Typically, it is possible to 'squeeze out' a few
more percentage points by doing so. This could be done with
\href{https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html}{\texttt{scipy.optimize.minimize}}.

Another possible improvement was already mentioned before: map
visualization and improved analysis could be carried on with latitude
and longitude coordinates, as done in many Kaggle
\href{https://www.kaggle.com/c/nyc-taxi-trip-duration/kernels}{kernels}
related with this competition. As we have chosen to merge the four
coordinates columns in a simple way to just one new feature, some
relevant information could be lost. The feature importance plot above
has showed us that the most relevant information comes from these
features.

    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
